# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session








import pickle 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns


from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier


from imblearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report 





Data = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')
Data.head()


Data.shape


# Display all columns 
pd.set_option('display.max_column', None)


Data.head()


# Information about the data 
Data.info()


# Drop "customer_ID" column (Not necessary for modelling)

Data = Data.drop("customerID", axis = 1)
Data.head()


Data.columns


# Unique values from all the features 

Num_col = ["tenure", "MonthlyCharges", "TotalCharges"]


# Only categorical features unique values were display
for col in Data.columns:
    if col not in Num_col:
        print(f"Columns: {col}")
        print("Unique Values:",Data[col].unique())
        print("-" * 60)


Data.isnull().sum()


# Empty values in "TotalCharges" column
Data[Data["TotalCharges"] == ' ']


len(Data[Data["TotalCharges"] == ' '])


# Replace the empty value 
Data["TotalCharges"] = Data["TotalCharges"].replace({' ': '0.0'})


# Convert "TotalCharges" column into "float"
Data["TotalCharges"] = Data["TotalCharges"].astype(float)


Data.info()


# Distribution of Target columnÂ¶
Data["Churn"].value_counts()











Data.shape


Data.columns 


Data.head()


Data.describe()  # Only work on numerical features








def plot_histogram(df, column_name):
    plt.figure(figsize = (5, 3))
    sns.histplot(df[column_name], kde = True)
    plt.title(f"Distribution of {column_name}")
    
    # Calculate mean and median values for the features
    mean_col = df[column_name].mean()
    median_col = df[column_name].median()

    # Add vertical lines for mean and median 
    plt.axvline(mean_col, color = 'red', linestyle = "--", label = "Mean")
    plt.axvline(median_col, color = 'green', linestyle = "-", label = "Median")

    plt.legend()
    plt.show()


plot_histogram(Data, "tenure")


plot_histogram(Data, "MonthlyCharges")


plot_histogram(Data, "TotalCharges")





def plot_boxplot(df, column_name):
    plt.figure(figsize = (5, 3))
    sns.boxplot(y = df[column_name])
    plt.title(f"Distribution of {column_name}")
    plt.ylabel(column_name)
    plt.show()


plot_boxplot(Data, "tenure")


plot_boxplot(Data, "MonthlyCharges")


plot_boxplot(Data, "TotalCharges")





plt.figure(figsize = (8, 4))
sns.color_palette("Spectral", as_cmap=True)
sns.heatmap(Data[["tenure", "MonthlyCharges", "TotalCharges"]].corr(), annot = True, fmt = ".2f" )
plt.title(f"Correlation Heatmap")
plt.show()











Data.columns





Cat_cols = Data.select_dtypes(include = "object").columns.to_list()

# Adding "senior citizen" feature in cat_cols.
Cat_cols = ["SeniorCitizen"] + Cat_cols


for col in Cat_cols:
    plt.figure(figsize = (5, 3))
    sns.countplot(x = Data[col])
    plt.title(f"Countplot of {col}")
    plt.xticks(rotation=45)
    plt.show()








Data.head()





Data["Churn"] = Data["Churn"].replace({"Yes": 1, "No": 0})


# Check 
Data["Churn"].value_counts()





Data.columns


# Identifying features with "object" data type 
Cat_cols = Data.select_dtypes(include = "object").columns


# Inititalize a dictionary to save the encoders
encoders = {}

for col in Cat_cols:
    label_encoder = LabelEncoder()
    Data[col] = label_encoder.fit_transform(Data[col])
    encoders[col] = label_encoder


# Save the encoders to pickle file 
with open("encoders.pkl", "wb") as f:
    pickle.dump(encoders, f)


encoders


Data.head()








# Seperate features into a independent and dependent 
X = Data.drop(columns = ["Churn"], axis = 1)
y = Data["Churn"]


X.head()


y.head()


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)


print(f"X_train shape: ",X_train.shape)
print(f"X_test shape: ", X_test.shape)
print(f"y_train shape: ", y_train.shape)
print(f"y_test shape: ", y_test.shape)


X_train.head()


X_test.head()








smote = SMOTE(random_state = 42)


# Only apply on training dataset not on testing dataset 
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)


print(f"X_train_smote shape: ", X_train_smote.shape)
print(f"y_train_smote shape: ", y_train_smote.shape)


X_train_smote.head()


y_train_smote.head()








# Training with default parameters 

models = {
    
    "Logistic Regression": Pipeline([
        ("scaler", StandardScaler()),
        ("model", LogisticRegression(max_iter=2000))
    ]),

    
    "SGD Classifier": Pipeline([
        ("scaler", StandardScaler()),
        ("model", SGDClassifier(random_state=42))
    ]),
    
    "Decision Tree": DecisionTreeClassifier(random_state = 42),
    
    "Random Forest": RandomForestClassifier(random_state = 42),
    
    "Exter Trees": ExtraTreesClassifier(random_state = 42),
    
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    
    "AdaBoost": AdaBoostClassifier(random_state=42),
    
    "KNN": Pipeline([
        ("scaler", StandardScaler()),
        ("model", KNeighborsClassifier())
    ]),
    
    "SVM": Pipeline([
        ("scaler", StandardScaler()),
        ("model", SVC(probability=True, random_state=42))
    ]),

    
    "XGBoost": XGBClassifier(
        random_state = 42, 
        eval_metric = "logloss", 
        use_label_encoder = False)
}





Baseline_results = []

for model_name, model in models.items():
    print(f"Training {model_name} (Baseline)")

    model.fit(X_train_smote, y_train_smote)
    y_pred = model.predict(X_test)


    acc = accuracy_score(y_test, y_pred)

    # Adding model name and accuracy
    Baseline_results.append({
        "Model": model_name,
        "Accuracy": acc
    })

    # Store results into a datarfame
    Baseline_df = pd.DataFrame(Baseline_results).sort_values(by = "Accuracy", ascending = False)



Baseline_df





skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Store cross validation results 
cv_scores = {}

# Perform 5-fold cross validation for each model 
for model_name, model in models.items():
    print(f"Training {model_name} with Stratified K-Fold CV")

    pipeline = Pipeline([
        ("scaler", StandardScaler()),
        ("smote", SMOTE(random_state=42)),
        ("model", model)
    ])if model_name in ["Logistic Regression", "SGD Classifier", "KNN", "SVM"] else Pipeline([
        ("smote", SMOTE(random_state=42)),
        ("model", model)
    ])

        
    scores = cross_val_score(
        pipeline, 
        X_train, 
        y_train, 
        cv = skf, 
        scoring = "accuracy")
    
    cv_scores[model_name] = scores 
    print(f"{model_name} cross-validation accuracy: {np.mean(scores):.4f}")
    print("-" * 75)





cv_results_df = pd.DataFrame({
    model: scores.mean()
    for model, scores in cv_scores.items()}.items(), columns = ["Model", "CV Accuracy"]).sort_values(
    by = "CV Accuracy", ascending = False
)

cv_results_df

















rf_pipeline = Pipeline([
    ("smote", SMOTE(random_state = 42)),
    ("rf", RandomForestClassifier(random_state = 42))
])


# Params store in dict 
rf_param_dict = {
    "rf__n_estimators": [100, 200, 300],
    "rf__max_depth": [None, 10, 20, 30],
    "rf__min_samples_split": [2, 5, 10],
    "rf__min_samples_leaf": [1, 2, 4],
    "rf__max_features": ["sqrt", "log2"]
}


# Apply randomzedsearchCV 
rf_search = RandomizedSearchCV(
    rf_pipeline,
    rf_param_dict,
    n_iter = 20,
    cv = skf,
    scoring = "roc_auc",
    n_jobs = -1,
    random_state = 42,
    verbose = 1
)

# Fit the model 
rf_search.fit(X_train, y_train)



print("Best RF Score:", rf_search.best_score_)
print("Best RF Params: ",rf_search.best_params_)





gb_pipeline = Pipeline([
    ("smote", SMOTE(random_state = 42)),
    ("gb", GradientBoostingClassifier(random_state = 42))
])


# Params store in dict 
gb_param_dist = {
    "gb__n_estimators": [100, 200, 300],
    "gb__learning_rate": [0.01, 0.05, 0.1],
    "gb__max_depth": [3, 5, 7],
    "gb__subsample": [0.8, 1.0]
}


# Apply randomzedsearchCV 
gb_search = RandomizedSearchCV(
    gb_pipeline,
    gb_param_dist,
    n_iter=20,
    cv=skf,
    scoring="roc_auc",
    n_jobs=-1,
    random_state = 42,
    verbose=1
)


# Fit the model 
gb_search.fit(X_train, y_train)


print("Best GB Score: ", gb_search.best_score_)
print("Best GB Params: ", gb_search.best_params_)





xgb_pipeline = Pipeline([
    ("smote", SMOTE(random_state=42)),
    ("xgb", XGBClassifier(
        random_state=42,
        eval_metric="logloss"
    ))
])


# Params store in dict 
xgb_param_dist = {
    "xgb__n_estimators": [100, 200, 300],
    "xgb__max_depth": [3, 5, 7],
    "xgb__learning_rate": [0.01, 0.05, 0.1],
    "xgb__subsample": [0.8, 1.0],
    "xgb__colsample_bytree": [0.8, 1.0]
}


# Apply randomzedsearchCV 
xgb_search = RandomizedSearchCV(
    xgb_pipeline,
    xgb_param_dist,
    n_iter=20,
    cv=skf,
    scoring="roc_auc",
    n_jobs=-1,
    random_state=42,
    verbose=1
)


# Fit the model 
xgb_search.fit(X_train, y_train)

print("Best XGB Score:", xgb_search.best_score_)
print("Best XGB Params:", xgb_search.best_params_)








tuned_results = pd.DataFrame({
    "Model": ["Random Forest", "Gradient Boosting", "XGBoost"],
    "CV Accuracy": [
        rf_search.best_score_,
        gb_search.best_score_,
        xgb_search.best_score_
    ]
}).sort_values(by = "CV Accuracy", ascending = False)


tuned_results


# Set the visual style
sns.set_theme(style="whitegrid")
plt.figure(figsize=(10, 6))

# Create the barplot
ax = sns.barplot(x="CV Accuracy", y="Model", data=tuned_results, palette="viridis", hue="Model", legend=False)

# Add titles and labels
plt.title("Comparison of Model Accuracy after Hyperparameter Tuning", fontsize=15)
plt.xlabel("Cross-Validated Accuracy", fontsize=12)
plt.ylabel("Model", fontsize=12)


# This helps visualize small gaps between 0.831 and 0.832
plt.xlim(tuned_results["CV Accuracy"].min() - 0.005, tuned_results["CV Accuracy"].max() + 0.005)

# Add the actual score labels on the bars
for i in ax.containers:
    ax.bar_label(i, fmt='%.4f', padding=5)

plt.tight_layout()
plt.show()








# Select the best model from the search
best_model = xgb_search.best_estimator_

# Make predictions on the test set
y_pred_xgb = best_model.predict(X_test)
y_prob_xgb = best_model.predict_proba(X_test)[:, 1]

# Comprehensive Evaluation
print("------------ Final XGBoost Evaluation (Tuned) ---------------")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")
print("\nConfusion Matrix:")
sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))


# save trained model as pickle file 
model_data = {"model": xgb_search.best_estimator_, "feature_name":X.columns.tolist()}

with open("Telco_customer_churn_train_model.pkl", "wb") as f:
    pickle.dump(model_data, f)








with open("Telco_customer_churn_train_model.pkl", "rb") as f:
    model_data = pickle.load(f)

    loaded_model = model_data["model"]
    feature_names = model_data["feature_name"]


loaded_model


feature_names


# Store input data into a dictionary 
input_data = {
    'gender': 'Female',
    'SeniorCitizen': 1,
    'Partner': 'No' ,
    'Dependents': 'No',
    'tenure': 2,
    'PhoneService': 'Yes',
    'MultipleLines': 'No',
    'InternetService': 'Fiber optic',
    'OnlineSecurity': 'No',
    'OnlineBackup': 'No',
    'DeviceProtection': 'No',
    'TechSupport': 'No',
    'StreamingTV': 'Yes',
    'StreamingMovies': 'No',
    'Contract': 'Month-to-month',
    'PaperlessBilling': 'Yes',
    'PaymentMethod': 'Electronic check',
    'MonthlyCharges': 85.65,
    'TotalCharges': 171.30
}

# Convert dict into a dataframe 
input_data_df = pd.DataFrame([input_data])


with open("encoders.pkl", "rb") as f:
    encoders = pickle.load(f)


for col, encoder in encoders.items():
    input_data_df[col] = encoder.transform(input_data_df[col])


print(input_data_df.head())


encoders


# Prediction 
prediction = loaded_model.predict(input_data_df)
pred_prob = loaded_model.predict_proba(input_data_df)
print(prediction)


# Result 
print(f"Prediction: {'Churn' if prediction[0] == 1 else 'No Churn'}")
print(f"Prediction Probability: {pred_prob}")






